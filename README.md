# dep-compose

This is a further development of a project that started as a class assignment. In it, I attempt to train a recursive neural net to compose phrase or sentence embeddings.

### Approach

The original project attempted to use a sentence embedding generated by a different sentence embedding generation method as the target, with word2vec embeddings and dependencies as the inputs. It selected a transformation based on the dependency type and used it on the concatenation of the head embedding and tail embedding. This was problematic not only for the obvious reason that it requires an existing sentence embedding method as a prerequisite and will achieve at best the usefulness of that method, but also because in each training instance the backpropagation must go through an entire parse. The relative data sparsity considering the effective depth of the network was a major problem, as was the variability of the effective batch sizes.

In this variation, the key difference is that the target embeddings will be those of small phrases. By restricting the training to small phrases and then extrapolating to full sentences, it is possible that some of these training issues will be minimized. If the target is the embedding of a bigram or trigram, it should be much easier to create that target, and moreover the same word2vec embedding method can be used.

The new challenge that this presents is the efficient generation of those ngram embeddings.

Part of the reason that the old version used word2vec in particular was that it grew out of an adaptation of an earlier project (my Bachelor's thesis) that looked at compatibility of constituents in a candidate phrase. That is arguably a syntactically heavier problem than this one, and one of the key differences between word2vec and, for instance, GloVe, is that word2vec encodes a relatively large amount of syntactic information due to its focus on proximity. However, now we mainly want semantics. Is word2vec still advantageous? This will be a new question that needs to be tested in conjunction with the rest of the changes in order to give the model a good chance at doing anything meaningful.

### Structure

One key difference between the two projects of which this one is a continuation, and between it and both of them, is where the graph generation and training are done. In the first project, because the networks were more or less feed-forward, the graph could easily be built in Python, saved, imported in the Rust program, and trained. In the second, due to project constraints, there was no Rust component anyway. Here, the idea was initially to do the graph generation in Python and the training in Rust, as with the first. However, this is not easily doable since the graph needs to be defined in terms of each individual input.

Unfortunately, due to restraints on how the Tensorflow 2/Keras library works, even the Keras Functional API is not suitable for dynamic graphs. Therefore this entire project is being scrapped and rewritten to use CMU's DyNet.

