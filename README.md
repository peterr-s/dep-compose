# dep-compose

This is a further development of a project that started as a class assignment. In it, I attempt to train a recursive neural net to compose phrase or sentence embeddings. Because it is now a personal project and I don't have to use Python, I will be rewriting large parts of it in Rust.

### Approach

The original project attempted to use a sentence embedding generated by a different sentence embedding generation method as the target, with word2vec embeddings and dependencies as the inputs. It selected a transformation based on the dependency type and used it on the concatenation of the head embedding and tail embedding. This was problematic not only for the obvious reason that it requires an existing sentence embedding method as a prerequisite and will achieve at best the usefulness of that method, but also because in each training instance the backpropagation must go through an entire parse. The relative data sparsity considering the effective depth of the network was a major problem, as was the variability of the effective batch sizes.

In this variation, the key difference is that the target embeddings will be those of small phrases. By restricting the training to small phrases and then extrapolating to full sentences, it is possible that some of these training issues will be minimized. If the target is the embedding of a bigram or trigram, it should be much easier to create that target, and moreover the same word2vec embedding method can be used.

The new challenge that this presents is the efficient generation of those ngram embeddings, and that will be the first focus.

Part of the reason that the old version used word2vec in particular was that it grew out of an adaptation of an earlier project (my Bachelor's thesis) that looked at compatibility of constituents in a candidate phrase. That is arguably a syntactically heavier problem than this one, and one of the key differences between word2vec and, for instance, GloVe, is that word2vec encodes a relatively large amount of syntactic information due to its focus on proximity. However, now we mainly want semantics. Is word2vec still advantageous? This will be a new question that needs to be tested in conjunction with the rest of the changes in order to give the model a good chance at doing anything meaningful.
